# -*- coding: utf-8 -*-
"""spam filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AB0WzWKR4nF8ojDv2H1GuLXR03wP6YwD
"""

!pip install gensim

import gensim
from gensim.models import Word2Vec, KeyedVectors

import pandas as pd

messages = pd.read_csv('/content/spam.csv',encoding='latin-1')

messages = messages[['v1', 'v2']]
messages.columns = ['label', 'message']
messages

import nltk
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('stopwords')

corpus_cont=[]

for i in range(0,len(messages)):
  review = re.sub('[^a-zA-Z]',' ',messages['message'][i])
  review = review.lower().split()
  lemmatizer = WordNetLemmatizer()
  review=[lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]
  review = ' '.join(review)
  corpus_cont.append(review)

print(corpus_cont)

words=[]
for i in range(0,len(corpus_cont)):
  tokenized_words = nltk.word_tokenize(corpus_cont[i])
  words.append(tokenized_words)

print(words)

modelw2v = gensim.models.Word2Vec(words,window=5,vector_size=100,min_count=2)

"""list of all the words (vocabulary) that the model learned during training."""

modelw2v.wv.index_to_key

modelw2v.epochs

modelw2v.wv.similar_by_word("gift")

modelw2v.wv['good'].shape

import numpy as np

def avg_word2vec(doc):
  return np.mean([modelw2v.wv[word] for word in doc if word in modelw2v.wv.index_to_key],axis=0)

X=[]
for i in range(len(words)):
  X.append(avg_word2vec(words[i]))

X

len(X)

X_new = np.array(X, dtype=object)

print(X_new[0].shape)
print(X_new.shape)

y= pd.get_dummies(messages['label'])
y =  y.iloc[:,0].values

X[0].reshape(1,-1).shape

df = pd.DataFrame()
for i in range(0,len(X)):
  df = pd.concat([df, pd.DataFrame(X[i].reshape(1, -1))], ignore_index=True)

df.shape

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

X_train

X_train_cleaned = [vec for vec in X_train if isinstance(vec, np.ndarray) and vec.shape == (100,)]
y_train_cleaned = [y_train[i] for i, vec in enumerate(X_train) if isinstance(vec, np.ndarray) and vec.shape == (100,)]

X_train = np.array(X_train_cleaned)
y_train = np.array(y_train_cleaned)

X_test_cleaned = [vec for vec in X_test if isinstance(vec, np.ndarray) and vec.shape == (100,)]
y_test_cleaned = [y_test[i] for i, vec in enumerate(X_test) if isinstance(vec, np.ndarray) and vec.shape == (100,)]

X_test = np.array(X_test_cleaned)
y_test = np.array(y_test_cleaned)

# Check what's in X_train
print("X_train type:", type(X_train))
print("X_train length:", len(X_train))

# Check individual elements
for i in range(min(5, len(X_train))):
    print(f"X_train[{i}] shape:",
          X_train[i].shape if hasattr(X_train[i], 'shape') else type(X_train[i]))

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test,y_pred)
print(score)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))